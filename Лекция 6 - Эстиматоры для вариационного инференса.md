**Ссылка на лекцию** - https://rutube.ru/play/embed/8635255d4df6d8bcffa66bd1df7166c5/?p=5v88yuLRXrZFdZxW6RJFyA

---

- **Решаем задачу вариационного инференса** – определяем вариационное распределение, обладающее нужными нам свойствами (простота, легкость получения семплов), которым можно с достаточной точностью заменить сложное апостериорное распределение.

- Часто, апостериорное распределение представляется в параметрическом  виде. То есть, мы задаем распределение как семейство неких функций с заданными параметрами.
	- Следовательно - наша задача, найти вариационное распределение, а следовательно, **определить параметры**, который задают то или иное распределение в семействе распределений.
	- Правдоподобие модели тоже может быть параметризовано.

- В процеcсе обучения **минимизируем дивергенцию Кульбака-Лейбнера**, которую мы напрямую рассчитать не можем, поэтому заменяем ее на:
$$ - \text{ELBO} \left( p_{\theta}(z|x), q_{\varphi}(z) \right)$$
---
### Score Sunction Estimator (REINFORSE):
**Основная формула:**

$$
\begin{align*}
\nabla_{\boldsymbol{\varphi}}\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})}
\left[
    \frac{\log p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})}{q_{\boldsymbol{\varphi}}(\mathbf{z})}
\right]
=
\mathbb{E}_{q_{\boldsymbol{\varphi}}(\mathbf{z})}
\left[
    \log\frac{p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})}{q_{\boldsymbol{\varphi}}(\mathbf{z})}
    \cdot
    \nabla_{\boldsymbol{\varphi}}\log q_{\boldsymbol{\varphi}}(\mathbf{z})
\right].
\end{align*}
$$
**И градиент**:
$$
\nabla_{\varphi} \text{ELBO} \approx \frac{1}{S} \sum_{s=1}^{S} \left[ \log \frac{p_{\theta}(\mathbf{x}, \mathbf{z}_{s})}{q_{\varphi}(\mathbf{z}_{s})} \cdot \nabla_{\varphi} \log q_{\varphi}(\mathbf{z}_{s}) \right].
$$
**Свойства данного эстиматора**:
- Несмещенный.
- Имеет большую дисперсию.
- Если какой то семпл $\mathbf{z}_{s}$ оказался в маловероятной области, то $q_{\varphi}(\mathbf{z}_{s})$ будет близко к **0** , а $\log(0) \to -\infty$ . Следовательно, небольшое изменение $\varphi$ приведет к большому изменению $\nabla_{\varphi} \log q_{\varphi}(\mathbf{z}_{s})$, так получаем большую дисперсию.  

---
### Эстиматор с репараметризацией (Pathwise)
Применим для большинства непрерывных распределений.
**Пусть** $$
\mathbf{z} \sim q_{\varphi}(\mathbf{z}), \quad 
\mathbf{z} = g(\boldsymbol{\varphi}, \varepsilon), \quad 
\varepsilon \sim \check{\xi}(\varepsilon),
$$**Пояснение:**
1. $\mathbf{z}$ семплируется из распределения $q_{\varphi}(\mathbf{z})$. И если мы захотим взять производную от $\mathbf{z}$, то сделать мы этого не сможем, так как $\mathbf{z}$ является семплом.
2. Представим $q_{\varphi}(\mathbf{z})$ в виде детерминированной функции $g(\varphi, \epsilon)$ с параметром $\varphi$ и случайной величиной $\epsilon$, которая от параметра не зависит. То есть, вместо того, чтобы семплировать из $q_{\varphi}(\mathbf{z})$, возьмем случайную величину $\varepsilon$ с определенной плотностью распределения и возьмем семпл из нее. Далее, этот семпл, с помощью некоторой детерминированной функции $g(\boldsymbol{\varphi}, \varepsilon)$ преобразуем так, чтобы он равнялся $\mathbf{z}$.
3. $\varepsilon$ от $\varphi$ не зависит, поэтому производную по его плотности тянуть не нужно. А от $\varphi$ зависит сама детерминированная функция, по которой мы сможем протянуть производную.

**Тогда эстиматор примет вид:**
$$
\nabla_{\!\boldsymbol{\varphi}} \mathbb{E}_{q_{\boldsymbol{\varphi}}(\mathbf{z})} 
\left[ 
    \frac{\log p_{\boldsymbol{\theta}}(\mathbf{x}, \mathbf{z})}{q_{\boldsymbol{\varphi}}(\mathbf{z})} 
\right] 
= \nabla_{\!\boldsymbol{\varphi}} \mathbb{E}_{\xi(\boldsymbol{\varepsilon})} 
\left[ 
    \frac{\log p_{\boldsymbol{\theta}}(\mathbf{x}, g(\boldsymbol{\varphi}, \boldsymbol{\varepsilon}))}{q_{\boldsymbol{\varphi}}(g(\boldsymbol{\varphi}, \boldsymbol{\varepsilon}))} 
\right] 
= \mathbb{E}_{\xi(\boldsymbol{\varepsilon})} 
\left[ 
    \nabla_{\!\boldsymbol{\varphi}} \frac{\log p_{\boldsymbol{\theta}}(\mathbf{x}, g(\boldsymbol{\varphi}, \boldsymbol{\varepsilon}))}{q_{\boldsymbol{\varphi}}(g(\boldsymbol{\varphi}, \boldsymbol{\varepsilon}))} 
\right]
$$

---
### Оценка эстиматоров

**Критерии для оценки качества работы эстиматора:**
- **Согласованность:** При увеличении количества семплов из распределения результат работы эстиматора должен к истинному значению матожидания ELBO.
- **Несмещенность**:  Результаты работы эстиматора должны являться центрированными относительно истинного значения матожидания ELBO.
- **Минимальное значение дисперсии:** Из всех эстиматоров, обрабатывающих одинаковое значение семплов, предпочтителен эстиматор **с наименьшим значением дисперсии.**
- **Эффективность:** Предпочтителен эстиматор, требующий **малого количества семплов** и способный **легко распараллеливаться**.

**Выбор эстиматора:**
- Использование одного из видов эстиматора, определяется, как правило, возможностью **репараметризации распределений**.
- В основном применяют **pathwise**, так как он имеет меньшую дисперсию. **Score function** применяется только если у распределения отсутствует эффективная репараметризация (*например, дискретные латентные переменные*).
- Чаще всего при расчете градиентов от ELBO применяют оба эстиматора, каждый для своего множества параметров:  
$$
\mathbf{z'} \sim q_{\varphi'}(\mathbf{z'}), \quad 
\mathbf{z''} \sim q_{\varphi''}(\mathbf{z''})$$
$$
\nabla_{\boldsymbol{\varphi}} \text{ELBO} = 
\begin{bmatrix}
\nabla_{\varphi'} \text{ELBO} \\
\nabla_{\varphi''} \text{ELBO}
\end{bmatrix}
$$

---
### Стохастический вычислительный граф

**Содержит следующие типы вершин:**
- **Входные** вершины $\theta$, включая параметры, по которым рассчитывается градиент.
- **Детерминированные вершины** $\mathcal{D}$, определяемые как детерминированные функции.
- **Стохастические вершины** $\mathcal{S}$, осуществляющие реализацию семплов из распределения.
- **Вершины функции ошибки** $\mathcal{C}$,  в которых заданы слагаемые функции ошибки (обычно ELBO)

***Стохастические** вершины обозначаются кругами, а **детерминированные** - прямоугольником.*
![[Pasted image 20250513142542.png]]

![[Pasted image 20250513142604.png]]

---
### Суррогатная функция

**Вопрос** - как тянуть градиент для расчета производных по $\varphi$ ?
Введем суррогатную функцию $L$ Чтобы рассчитать градиент от ***ELBO***. Теперь градиент от матожидания суммы всех составляющих равен градиенту матожидания суррогатной функции.
$$
\begin{align*}
\nabla_{\boldsymbol{\varphi}} \text{ELBO} = \nabla_{\boldsymbol{\varphi}}\mathbb{E} \left[ \sum_{c\in\mathcal{C}} c \right] = \mathbb{E} \left[ \nabla_{\boldsymbol{\varphi}} {L}(\theta, \mathcal{S}) \right]
\end{align*}
$$
Определим **суррогатную функцию**:
$$
{L}(\theta, \mathcal{S}) = \sum_{w \in \mathcal{S}} \log p(w \mid \text{DEPS}_w) \cdot \hat{Q} + \sum_{c \in \mathcal{C}} c(\text{DEPS}_c)
$$
- $v \prec^D w$ - детерминированное влияние $v$ на $w$
- $v \prec w$ — влияние $v$ на $w$
- $\text{DEPS}_c = \{w \in \theta \cup \mathcal{S} : w \prec^D c\}$ — Множество зависимостей элемента $\mathcal{c}$ — это все элементы $\mathcal{w}$ из объединения множеств $\theta$  и $\mathcal{S}$, такие что $\mathcal{w}$ зависит от $\mathcal{c}$ в соответствии с отношением $\prec^D$

- **Первое** слагаемое относится к **стохастическим** узлам.
	- $\hat{Q}$ можно представить как сумму всех ошибок, по сути ***ELBO***. *Шляпка* означает, что при дифференцировании данная величина будет *константой*. То есть, мы считаем  ***ELBO*** константой и берем производную только от величины $\log p(w \mid \text{DEPS}_w)$.
$$
\hat{Q} = \sum_{\substack{C \in \mathcal{C} \\ w \prec c}} c
$$
	- Как правило, считать все ***ELBO*** чтобы протянуть производную по всем стохастическим вершинам $w \in S$ не нужно, так как каждая вершина влияет на часть ***ELBO***, **нужно рассчитать только ту часть, на которую она влияет**. Данный подход требует учет структуры графа. То есть, рассчитываем $\hat{Q}$ только по тем вершинам, которые оказывают влияние $w \prec с$ на соответствующую часть функции ошибки.
- **Второе** - к **детерминированным**.

Таким образом, **если в графе нет ни одного стохастического узла**, по которому мы бы хотели протянуть градиент, **то учитывается 2е слагаемое** (сумма всех ошибок), по которому не составляет труда протянуть градиент. Если же **стохастические вершины есть**, то **учитывается 1е слагаемое**. 

---
### Теорема Рао-Блеквелла

**Учет структуры графа** при расчете суррогатной функции позволяет применить теорему **Рао-Блеквелла**:

>Пусть $\hat\theta_n$ – несмещенный эстиматор некоторого параметра $\boldsymbol{\theta} \in \Theta$, и пусть  $𝑇(𝐗)$ – эффективная статистика для $\boldsymbol{\theta}$. Тогда:
>- $\theta_n = \mathbb{E}\left[\hat{\theta}_n \mid T(\mathbf{X})\right]$ - тоже несмещенный эстиматор $\boldsymbol{\theta}$,
>- $D_{\theta} \tilde{\theta}_n \leq D_{\theta} \hat{\theta}_n \quad \forall \boldsymbol{\theta}.$ - Дисперсия оценки $\tilde\theta_n$​ не превышает дисперсии $\hat\theta_n$​ для любого истинного $\theta$.

>*Имеем некоторый эстиматор $\hat\theta_n$, обладающий определенной энергией. Вопрос - можно ли эту дисперсию уменьшить? Можно, если мы будем место данного эстиматора $\hat\theta_n$, использовать эстиматор по некоторой величине $\mathbb{E}\left[\hat{\theta}_n \mid T(\mathbf{X})\right]$, по сути делаем этот эстиматор условным по каким либо доп. данным $\mathbf{X}$ и дисперсия станет меньше первоначальной.* 

Под **эффективной статистикой** понимают функцию выборки, применение которой к оценке параметра будет **столь же эффективным**, как всей выборки. 
- Используя данную теорему, можно показать, что рассмотренный ранее эстиматор на основе стохастического графа имеет меньшую дисперсию.  
- Такой подход к уменьшению дисперсии называется Rao-Blackwellization

Данная теорема позволяет утверждать, что **учет структуры графа** (прокидывание производной только по целевому множеству данных) позволяет получить **эстиматор с меньшей дисперсией**.

---
### Пример

Имеем **стохастический** граф:
![[Pasted image 20250515170329.png]]

Преобразуем его к **детерминированному**:
![[Pasted image 20250515170455.png]]

**Тут:**
1. Проводим репараметризацию:
- ![[Pasted image 20250515170653.png]]
2. От $\boldsymbol{z}''$ не получится взять производную, поэтому **вводим суррогатную функцию ошибки**:
	- *Первая часть* суррогатной функции. По ней не тянем производную.
		- ![[Pasted image 20250515171203.png]]
	- *Вторая часть*. Тянем производную.
		- ![[Pasted image 20250515170933.png]]

---
### Способ уменьшения дисперсии эстиматора REINFORCE

**Трюк в отнимании $\beta$ (в скобках).**

Введем следующее расширение эстиматора **REINFORCE** $\mathcal{F}$, добавив  константу $\beta(по \quad  \phi)$:
$$
\widehat{\mathcal{F}} = \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})}
\left[
  \left(
    \log\frac{p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z})} - \beta
  \right)
  \cdot
  \nabla_{\boldsymbol{\phi}} \log q_{\boldsymbol{\phi}}(\mathbf{z})
\right]
$$

- После преобразований и представлений видим, что он эквивалентен REINFORCE для $\forall \beta$ 
$$
\begin{align*}
\widehat{\mathcal{F}} = 
&\mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})}
\left[
  \log\frac{p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z})}
  \cdot
  \nabla_{\boldsymbol{\phi}} \log q_{\boldsymbol{\phi}}(\mathbf{z})
\right] 
- \beta \cdot \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})}
\bigl[
  \nabla_{\boldsymbol{\phi}} \log q_{\boldsymbol{\phi}}(\mathbf{z})
\bigr]
\end{align*}
$$
	- Вычитаемое преобразуется в $0$.
$$
\widehat{\mathcal{F}} = \mathbb{E}_{q_{\boldsymbol{\phi}}(\mathbf{z})} \Bigg[ 
    \log\frac{p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})}{q_{\boldsymbol{\phi}}(\mathbf{z})} 
    \cdot 
    \nabla_{\boldsymbol{\phi}} \log q_{\boldsymbol{\phi}}(\mathbf{z})
\Bigg]
$$

- Переменные 𝛽 называются **baselines**. Можно показать, что правильный подбор $\beta$ **снижает дисперсию эстиматора**, оставляя его **несмещённым**.

Подходы к подбору **baselines**:
1. g